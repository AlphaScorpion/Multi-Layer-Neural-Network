{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafb78be",
   "metadata": {},
   "source": [
    "<b> Name: Aishwarya Bhavsar </b><br>\n",
    "<b> CSULB ID: 029371509 </b><br>\n",
    "<b>CECS 551 - Assignment 3 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e14fd1",
   "metadata": {},
   "source": [
    " <b> Implement multi-layer neural network WITHOUT using external deep learning li\u0002braries such as Keras, Caffe, Theano, TensorFlow, PyTorch </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d8fe14",
   "metadata": {},
   "source": [
    "<b>PART (a) </b><br>\n",
    "\n",
    "(a) Consider a neural network as shown in Figure 1 that approximates XOR function. <br>\n",
    "• The width of the layer 1 is 2, and the width of the layer 2 is 1.<br>\n",
    "• The activation functions of the layer 1 are the hyperbolic tangent.<br>\n",
    "• The activation function of the layer 2 is the sigmoid.<br>\n",
    "• The loss function is the binary cross entropy <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc097f7",
   "metadata": {},
   "source": [
    "<b>Import Numpy Library <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "477051a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Python Libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d3e44c",
   "metadata": {},
   "source": [
    "<b> Hyperbolic Tangent Function - For Hidden Layer 1 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "6b9eddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of activation function of the layer 1\n",
    "#hyperbolic tangent function\n",
    "def tanh(x):\n",
    "    return (1.0 - np.exp(-2*x))/(1.0 + np.exp(-2*x))\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 + x)*(1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1bfa55",
   "metadata": {},
   "source": [
    "<b> Sigmoid Function - For Hidden Layer 2 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "52a42b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of activation function of the layer 2\n",
    "# Sigmoid Function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "def sigmoid_derivative(z):\n",
    "    return y * (1 - z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1086cc",
   "metadata": {},
   "source": [
    "<B>THE GRADIENT DESCENT METHOD TO OPTIMISE THE PARAMETERS</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "533dae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of gradient descent method\n",
    "def gradient_descent(\n",
    "    gradient, start, learn_rate, n_iter=50, tolerance=1e-06\n",
    "):\n",
    "    vector = start\n",
    "    for _ in range(n_iter):\n",
    "        diff = -learn_rate * gradient(vector)\n",
    "        if np.all(np.abs(diff) <= tolerance):\n",
    "            break\n",
    "        vector += diff\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "e1ebccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the neural network parameters\n",
    "# Initialized all the weights in the range of between 0 and 1\n",
    "# Bias values are initialized to 0\n",
    "\n",
    "def initializeParameters(modelinput, total_neurons, modeloutput):\n",
    "    W = np.random.randn(total_neurons, modelinput)\n",
    "    w = np.random.randn(modeloutput, total_neurons)\n",
    "    b1 = np.zeros((total_neurons, 1))\n",
    "    b2 = np.zeros((modeloutput, 1))\n",
    "    \n",
    "    parameters = {\"W\" : W, \"b1\": b1,\n",
    "                    \"w\" : w, \"b2\": b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6b92f7",
   "metadata": {},
   "source": [
    "<b>Part (c)</b> <br>\n",
    "Implement the model without using any deep learning libraries. However, you can use import numpy in case you need.<br>\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]]) <br>\n",
    "y = np.array([0,1,1,0])<br>\n",
    "You need to optimize the parameters using simple gradient descent method. <br>\n",
    "Predict ˆy using the trained network and show the result. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8167cd0d",
   "metadata": {},
   "source": [
    "<b> Forward Propagation </b>- The input data is fed in the forward direction through the network.<br> Each hidden layer accepts the input data, processes it as per the activation function and passes to the successive layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "23fb8cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Propagation\n",
    "def forwardPropagation(X, Y, parameters):\n",
    "    m = X.shape[1]\n",
    "    W = parameters[\"W\"]\n",
    "    w = parameters[\"w\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    Z1 = np.dot(W, X) + b1\n",
    "    A1 = tanh(Z1)\n",
    "    Z2 = np.dot(w, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    store = (Z1, A1, W, b1, Z2, A2, w, b2)\n",
    "    loss_log = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y))\n",
    "    cost_function = -np.sum(loss_log) / m    \n",
    "    return cost_function, store, A2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c27d5",
   "metadata": {},
   "source": [
    "<b> Backward Propagation </b> - Fine-tuning the weights of a neural net based on the error rate (i.e., loss) obtained in the previous epoch (i.e., iteration). <br> Proper tuning of the weights ensures lower error rates, making the model reliable by increasing its generalization. \n",
    "\n",
    "<b>Mini-Batches</b>\n",
    "The network is designed to process examples in mini-batches.<br>\n",
    "The gradients are calculated for more than one example at a time. <br>\n",
    "The sum is squashing the results down to a single update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "7e2bd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Propagation\n",
    "def backwardPropagation(X, Y, store):\n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W, b1, Z2, A2, w, b2) = store\n",
    "\n",
    "    dZ2 = A2 - Y\n",
    "    dw = np.dot(dZ2, A1.T) / m\n",
    "    #matrix multiplication of the local and the upstream derivatives\n",
    "    db2 = np.sum(dZ2, axis = 1, keepdims = True)  #dimensions of the array matrix gets saved.\n",
    "\n",
    "    dA1 = np.dot(w.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, A1 * (1- A1))\n",
    "    dW = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis = 1, keepdims = True) / m\n",
    "        \n",
    "    gradients = {\"dZ2\": dZ2, \"dw\": dw, \"db2\": db2,\n",
    "                \"dZ1\": dZ1, \"dW\": dW, \"db1\": db1}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d104e139",
   "metadata": {},
   "source": [
    "<b>Updation Of Weights </b>: Weights associated with neuron connections must be updated after forward passes of data through the network. <br> These weights are adjusted to help reconcile the differences between the actual and predicted outcomes for subsequent forward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "d9dde88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the weights based on the negative gradients\n",
    "def updateParameters(parameters, gradients, learningRate):\n",
    "    parameters[\"W\"] = parameters[\"W\"] - learningRate * gradients[\"dW\"]\n",
    "    parameters[\"w\"] = parameters[\"w\"] - learningRate * gradients[\"dw\"]\n",
    "    parameters[\"b1\"] = parameters[\"b1\"] - learningRate * gradients[\"db1\"]\n",
    "    parameters[\"b2\"] = parameters[\"b2\"] - learningRate * gradients[\"db2\"]\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf75b2",
   "metadata": {},
   "source": [
    "<b> XOR Inputs </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "9d1f5c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to learn the XOR truth table\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]]) # XOR input\n",
    "Y = np.array([[0, 1, 1, 0]])               # XOR output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d745c2a3",
   "metadata": {},
   "source": [
    "$$Learning Rate (\\eta) $$\n",
    "\n",
    "As mentioned in the question, if there is only time to optimize one hyper-parameter and one uses stochastic gradient descent, then this is the hyper-parameter (learning rate) that is worth tuning.<br>\n",
    "A default good learning rate is 0.1 or 0.01 based on trial and error method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "3ddd2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "total_neurons = 3 # number of hidden layer neurons (2)\n",
    "modelinput = X.shape[0] # number of input features (2)\n",
    "modeloutput = Y.shape[0] # number of output features (1)\n",
    "parameters = initializeParameters(modelinput, total_neurons, modeloutput)\n",
    "epoch = 100000\n",
    "learningRate = 0.01\n",
    "losses = np.zeros((epoch, 1))\n",
    "\n",
    "for i in range(epoch):\n",
    "    losses[i, 0], store, A2 = forwardPropagation(X, Y, parameters)\n",
    "    gradients = backwardPropagation(X, Y, store)\n",
    "    parameters = updateParameters(parameters, gradients, learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb258670",
   "metadata": {},
   "source": [
    "<b>Performance Evaluation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "4bf6bcb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"LOSS VALUE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca824ad",
   "metadata": {},
   "source": [
    "<b>Test - Predict y(hat) using the trained network</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "7c2aa604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "X = np.array([[1, 1, 0, 0], [0, 1, 0, 1]]) # XOR input\n",
    "cost,_, A2 = forwardPropagation(X, Y, parameters)\n",
    "prediction = (A2 > 0.5) * 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeae3c56",
   "metadata": {},
   "source": [
    "<b> Output Results</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "d1b90656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# print(A2)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a52a0a",
   "metadata": {},
   "source": [
    "<b>PART (b) </b><br>\n",
    "Calculate derivates. Please include the answers in the Jupyter notebook.<br>\n",
    "Notice that you can use LATEX equation in the Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fefc2d7",
   "metadata": {},
   "source": [
    "<b>Simple Gradient Descent For Backpropagation</b> <br>\n",
    "All the weight parameters of the model for both the layers - Layer 1 & Layer 2<br>\n",
    "\n",
    "<b>Layer 2 Parameters:<b>\n",
    "\n",
    "$$ \\frac{{\\partial L}}{{\\partial w _{11}^{(2)}}} = \\left( {\\frac{{\\partial L}}{{\\partial {\\rm{ŷ}} _1^{}}}} \\right)\\left( {\\frac{{\\partial {\\rm{ŷ}} _1^{}}}{{\\partial h _1^{(2)}}}} \\right)\\left( {\\frac{{\\partial h _1^{(2)}}}{{\\partial  a_{1}^{(2)}}}} \\right) \\left({\\frac{{\\partial  a_{1}^{(2)}}}{{\\partial w _{11}^{(2)}}}}\\right)$$\n",
    "\n",
    "$$  =  {{{(ŷ _1 - y _1)(h _1^{(2)}) (1 - h _1^{(2)})(h _1^{(1)}) }}} $$\n",
    "$$  =  {{{(ŷ _1 - y _1)(ŷ _1) (1 - ŷ _1)(h _1^{(1)}) }}} $$\n",
    "\n",
    "\n",
    "$$ \\frac{{\\partial L}}{{\\partial w _{21}^{(2)}}} = \\left( {\\frac{{\\partial L}}{{\\partial {\\rm{ŷ}} _1^{}}}} \\right)\\left( {\\frac{{\\partial {\\rm{ŷ}} _1^{}}}{{\\partial h _1^{(2)}}}} \\right)\\left( {\\frac{{\\partial h _1^{(2)}}}{{\\partial  a_{1}^{(2)}}}} \\right) \\left({\\frac{{\\partial  a_{1}^{(2)}}}{{\\partial w _{21}^{(2)}}}}\\right)$$\n",
    "\n",
    "$$  =  {{{(ŷ _1 - y _1)(h _1^{(2)}) (1 - h _1^{(2)})(h _1^{(1)}) }}} $$\n",
    "$$  =  {{{(ŷ _1 - y _1)(ŷ _1) (1 - ŷ _1)(h _1^{(1)}) }}} $$\n",
    "\n",
    "<b>Optimisation Of Layer 2 Parameters :<b> <br>\n",
    "$$\\eta = 0.01$$\n",
    "\n",
    "$$w _{11}^{(2)} \\leftarrow w _{11}^{(2)} - \\eta \\frac{{\\partial L}}{{\\partial w _{11}^{(2)}}}$$ <br>\n",
    "$$w _{21}^{(2)} \\leftarrow w _{21}^{(2)} - \\eta \\frac{{\\partial L}}{{\\partial w _{21}^{(2)}}}$$\n",
    "\n",
    "\n",
    "<b>Layer 1 Parameters:<b>\n",
    "\n",
    "$$ \\frac{{\\partial L}}{{\\partial w _{11}^{(1)}}} = \\left( {\\frac{{\\partial L}}{{\\partial {\\rm{h}} _1^{(1)}}}} \\right)\\left( {\\frac{{\\partial {\\rm{h}} _1^{(1)}}}{{\\partial a _1^{(1)}}}} \\right)\\left( {\\frac{{\\partial a _1^{(1)}}}{{\\partial  w_{11}^{(1)}}}} \\right)$$\n",
    "\n",
    "$$={{{((ŷ _1 - y _1)ŷ _1(1 - ŷ _1)w _{11}^{(2)}) h _1^{(1)} (1 - h _1^{(1)})x _1 }}} $$ <br>\n",
    "\n",
    "$$ \\frac{{\\partial L}}{{\\partial w _{12}^{(1)}}} = \\left( {\\frac{{\\partial L}}{{\\partial {\\rm{h}} _2^{(1)}}}} \\right)\\left( {\\frac{{\\partial {\\rm{h}} _2^{(1)}}}{{\\partial a _2^{(1)}}}} \\right)\\left( {\\frac{{\\partial a _2^{(1)}}}{{\\partial  w_{12}^{(1)}}}} \\right)$$\n",
    "\n",
    "$$={{{((ŷ _1 - y _1)ŷ _1(1 - ŷ _1)w _{21}^{(2)}) h _2^{(1)} (1 - h _2^{(1)})x _1 }}} $$ <br>\n",
    "\n",
    "$$ \\frac{{\\partial L}}{{\\partial w _{21}^{(1)}}} = \\left( {\\frac{{\\partial L}}{{\\partial {\\rm{h}} _1^{(1)}}}} \\right)\\left( {\\frac{{\\partial {\\rm{h}} _1^{(1)}}}{{\\partial a _1^{(1)}}}} \\right)\\left( {\\frac{{\\partial a _1^{(1)}}}{{\\partial  w_{21}^{(1)}}}} \\right)$$\n",
    "\n",
    "$$={{{((ŷ _1 - y _1)ŷ _1(1 - ŷ _1)w _{11}^{(2)}) h _1^{(1)} (1 - h _1^{(1)})x _2 }}} $$ <br>\n",
    "\n",
    "$$ \\frac{{\\partial L}}{{\\partial w _{22}^{(1)}}} = \\left( {\\frac{{\\partial L}}{{\\partial {\\rm{h}} _2^{(1)}}}} \\right)\\left( {\\frac{{\\partial {\\rm{h}} _2^{(1)}}}{{\\partial a _2^{(1)}}}} \\right)\\left( {\\frac{{\\partial a _2^{(1)}}}{{\\partial  w_{22}^{(1)}}}} \\right)$$\n",
    "\n",
    "$$={{{((ŷ _1 - y _1)ŷ _1(1 - ŷ _1)w _{21}^{(2)}) h _2^{(1)} (1 - h _2^{(1)})x _2 }}} $$ <br>\n",
    "\n",
    "<b>Optimisation Of Layer 1 Parameters :<b> <br>\n",
    "$$\\eta = 0.01$$\n",
    "\n",
    "$$w _{11}^{(1)} \\leftarrow w _{11}^{(1)} - \\eta \\frac{{\\partial L}}{{\\partial w _{11}^{(1)}}}$$ <br>\n",
    "$$w _{11}^{(2)} \\leftarrow w _{11}^{(2)} - \\eta \\frac{{\\partial L}}{{\\partial w _{11}^{(2)}}}$$ <br>\n",
    "$$w _{21}^{(1)} \\leftarrow w _{21}^{(1)} - \\eta \\frac{{\\partial L}}{{\\partial w _{21}^{(1)}}}$$ <br>\n",
    "$$w _{22}^{(1)} \\leftarrow w _{22}^{(1)} - \\eta \\frac{{\\partial L}}{{\\partial w _{22}^{(1)}}}$$ <br>\n",
    "\n",
    "\n",
    "<b>Bias Parameters Of The Model:<b>\n",
    "\n",
    "$$ \\frac{{\\partial L}}{{\\partial b _{}^{(2)}}} = \\left( {\\frac{{\\partial L}}{{\\partial {\\rm{ŷ}} _1^{}}}} \\right)\\left( {\\frac{{\\partial {\\rm{ŷ}} _1^{}}}{{\\partial h _1^{(2)}}}} \\right)\\left( {\\frac{{\\partial h _1^{(2)}}}{{\\partial  a_{1}^{(2)}}}} \\right) \\left({\\frac{{\\partial  a_{1}^{(2)}}}{{\\partial b _{}^{(2)}}}}\\right)$$\n",
    "\n",
    "$$ \\frac{{\\partial L}}{{\\partial b _{1}^{(1)}}} = \\left( {\\frac{{\\partial L}}{{\\partial {\\rm{h}} _1^{(1)}}}} \\right)\\left( {\\frac{{\\partial {\\rm{h}} _1^{(1)}}}{{\\partial a _1^{(1)}}}} \\right)\\left( {\\frac{{\\partial a _1^{(1)}}}{{\\partial  b_{1}^{(1)}}}} \\right)$$\n",
    "\n",
    "$$ \\frac{{\\partial L}}{{\\partial b _{2}^{(1)}}} = \\left( {\\frac{{\\partial L}}{{\\partial {\\rm{h}} _2^{(1)}}}} \\right)\\left( {\\frac{{\\partial {\\rm{h}} _2^{(1)}}}{{\\partial a _2^{(1)}}}} \\right)\\left( {\\frac{{\\partial a _2^{(1)}}}{{\\partial  b_{2}^{(1)}}}} \\right)$$\n",
    "\n",
    "<b>Optimisation Of Bias Parameters equals to 1:<b> <br>\n",
    "$$\\eta = 0.01$$\n",
    "\n",
    "$$b _{}^{(2)} \\leftarrow b _{}^{(2)} - \\eta \\frac{{\\partial L}}{{\\partial b _{}^{(2)}}}$$ <br>\n",
    "$$b _{1}^{(1)} \\leftarrow b _{1}^{(1)} - \\eta \\frac{{\\partial L}}{{\\partial b _{1}^{(2)}}}$$ <br>\n",
    "$$b _{2}^{(1)} \\leftarrow b _{2}^{(1)} - \\eta \\frac{{\\partial L}}{{\\partial b _{2}^{(1)}}}$$ <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6929460",
   "metadata": {},
   "source": [
    "<b> Solution to XOR Problem - A Linear Model</b>\n",
    "\n",
    "$$\\frac{{\\partial L}}{{\\partial \\overrightarrow W _{}^{(1)}}}=\\begin{bmatrix} \\frac{{\\partial L}}{{\\partial w _{11}^{(1)}}} &  \\frac{{\\partial L}}{{\\partial w _{12}^{(1)}}} \\\\ \\frac{{\\partial L}}{{\\partial w _{21}^{(1)}}} &  \\frac{{\\partial L}}{{\\partial w _{22}^{(1)}}} \\end{bmatrix} $$\n",
    "$$=\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}$$\n",
    "\n",
    "$$ f(\\overrightarrow x;\\overrightarrow W,\\overrightarrow b _{}^{(1)},\\overrightarrow w,\\overrightarrow b _{}^{(2)})=\\overrightarrow w _{}^{(T)}\\overrightarrow h + \\overrightarrow b _{}^{(2)}$$\n",
    "\n",
    "$$=\\overrightarrow w _{}^{(T)}g(\\overrightarrow W _{}^{(T)}\\overrightarrow x \\overrightarrow b _{}^{(2)}+ \\overrightarrow b _{}^{(1)}) + \\overrightarrow b _{}^{(2)}$$\n",
    "\n",
    "$$=\\overrightarrow W _{}^{(T)}max(0, \\overrightarrow W _{}^{(T)}\\overrightarrow x + \\overrightarrow b _{}^{(1)}) + \\overrightarrow b _{}^{(2)}$$\n",
    "\n",
    "$$\\overrightarrow W _{}^{(T)}=\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} $$ <br>\n",
    "$$\\overrightarrow b _{}^{(1)}=\\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix} $$ <br>\n",
    "$$\\overrightarrow w _{}^{}=\\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix} $$ <br>\n",
    "$$b = 0$$<br>\n",
    "\n",
    "$$\\overrightarrow X _{}^{(T)}=\\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{bmatrix} $$ <br>\n",
    "$$\\overrightarrow X _{}^{(T)}\\overrightarrow W _{}^{(T)}=\\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{bmatrix}\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} $$ <br>\n",
    "$$=\\begin{bmatrix} 0 & 0 \\\\ 1 & 1 \\\\ 1 & 1 \\\\ 2 & 2 \\end{bmatrix}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674c8894",
   "metadata": {},
   "source": [
    "<b>Broadcasting</b><br>\n",
    "The term broadcasting refers to how numpy treats arrays with different Dimension during arithmetic operations which lead to certain constraints, the smaller array is broadcast across the larger array so that they have compatible shapes. \n",
    "\n",
    "$$\\overrightarrow W _{}^{(T)}\\overrightarrow x _{}^{} + \\overrightarrow b _{}^{(1)} = \\overrightarrow W _{}^{(}\\overrightarrow x _{}^{} + \\overrightarrow b _{}^{(1)}$$ <br>\n",
    "\n",
    "$$=\\begin{bmatrix} 0 & 0 \\\\ 1 & 1 \\\\ 1 & 1 \\\\ 2 & 2 \\end{bmatrix} + \\begin{bmatrix} 0 & -1 \\\\ 0 & -1 \\\\ 0 & -1 \\\\ 0 & -1 \\end{bmatrix}$$\n",
    "\n",
    "$$=\\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\\\ 1 & 0 \\\\ 2 & 1 \\end{bmatrix}$$\n",
    "\n",
    "$$max(0, \\overrightarrow W _{}^{}\\overrightarrow x + \\overrightarrow X _{}^{}) + \\overrightarrow b _{}^{(1)}$$ <br>\n",
    "$$=\\begin{bmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 1 & 0 \\\\ 2 & 1 \\end{bmatrix}$$ <br>\n",
    "\n",
    "$$max(0, \\overrightarrow W _{}^{}\\overrightarrow X + \\overrightarrow b _{}^{(1)})\\overrightarrow w + \\overrightarrow b _{}^{(2)}$$ <br>\n",
    "\n",
    "$$=\\begin{bmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 1 & 0 \\\\ 2 & 1 \\end{bmatrix}\\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix} + 0 $$ <br>\n",
    "$$=\\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0562c9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
